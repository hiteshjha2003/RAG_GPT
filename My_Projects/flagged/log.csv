Upload File,File Content,flag,username,timestamp
flagged\Upload File\7f1c640ebeb84c900892\White Paper.pdf,"('APPROACH AND RESULTS \n \nIntroduction \nFigure 1 shows the overall approach to build and visualise the machine learning model. A primary input to \nthe process is Distributed Control System (DCS) Historian data provided in the form of a large CSV files.The \ntime-series data set consisted of 422,449 rows with each row having 106 columns and covered a time \nperiod of approximately 4-years (January 2015 - January 2019) with temporal resolution of 5-minutes. \nIn order to assess equipment utilisation beyond capacity, additional equipment rating data was sourced \nfrom electrical power distribution system records. \nThe following sections describe in further detail the specific activities undertaken across the data \npreparation, model training, model testing, model optimisation, deployment and visualisation phases. \n \nAPPROACH \nData Preparation \nBefore the data could be used for training the models, the data was inspected for quality (missing values \netc), as well as the presence of key input variables well understood to likely drive the process plant \nbehaviour.   \nDomain Subject Matter Experts (SMEs) recommended 14 key input variables, also referred to as a feature \nset, which were likely to influence the performance of the operating plant and hence drive equipment power \nconsumption. The key input variables identified represent plant inflow (available at 5-minute intervals) as \nwell as flow quality parameters such as COD, NH3, Turbidity,  pH among others (available once every one \nor two days).  \nThere were a total of 91 output variables (labels) describing equipment power consumption all of which \nwere generally available at 5-minute intervals. The power consumption data was available in different forms \n(Watts, amps, VA etc) which were normalised during model development to enable comparison with with \nequipment rating data.  \nBoth the input feature set and output labels contained missing values for some time instances.  As the data \nis continuously changing in time series data sets, the missing feature values were substituted with the last \navailable value of the feature preceding the missing value. When training and testing machine learning \nmodels, it is important to maintain the integrity of the output labels to obtain valid results. For an output \nlabel, instances with missing values were discarded for both model training and testing. \nTo enable the prediction of the plant state 60minutes in advance, a commonly technique is to train the \nmodel utilising lagged data sets. To achieve this, a new data set lagged by a defined period of time (60min-\n115min) is created as the basis for training this use-case specific model. Within this new data set, the output \nlabels also become features for the purpose predicting future plant states. \nModel Development \nModel development involved three main activities: training the model, testing the model and model \noptimization. \nTraining and Testing Data \nThe data set was divided into a training and testing data set. The data set was divided into monthly datasets. \nEach of the monthly datasets was then further divided into training and testing sets using a 90/10 split: 90% \nof data for training and 10% for testing. That resulted in data from the first 27 days of a month being in the \ntraining set and the remaining data in the testing set. The reasoning behind splitting the data set this way \nis to eliminate the introduction biasing of recent data over older data. A model was trained and tested for \neach month and for each output variable. \nDefining an Appropriate Performance Metric \nThe commonly used coefficient of determination (R2) was used to assess model performance. The \ncoefficient of determination is used to assess how well the model predicts the real behaviour of the \nsystem. An R2 value of 0 indicates that the model does not explain any of the system behaviour, whilst a \nvalue of 1 indicates the model perfectly predicts the system behaviour.  \nAssessment of Co-Variance Influences \nBefore model training, it was necessary to determine which of the input variables had high covariances with \nthe output variables. To assess this, Pearson\'s R (correlation coefficient) was calculated for each input-\noutput pair. This did not yield many input/outputs pairs exhibiting a high Pearson\'s R correlation coefficient. \nTable 1 shows the top correlations found between input and output variables. \n \nTable 1: Correlation Between Input and Output Variables \n \nAs shown in Table 1, there was no significant relationships identified between specific input-output pairs. \nTo proceed with the modelling exericse, addition input was sought from SMEs to identify any additional \nfeatures that could be utilised to improve the peformance. The SMEs suggested that due to the nature and \narrangement of equipment items into ""banks"", it may be necessary to describe these groups as single \nvariables to  be modelled (by summation) rather than modelling individually on their own (for example banks \nof Direct On-Line loads). \n \nTable 2: Correlation Results from Summing Input and Output \n \n \nThe results in Table 2 show a significant improvement in the correlation values compared to the correlations \nshown in Table 1. \nModel Training \nAs distinct from the Model Performance (R2), the first model  trained was a linear regression model (line-\nof-best fit). Table 3 provides a summary of Performance Metric (R2) results of the regression models for all \nof the output variables across the testing data set. The results are summarised using standard statistical \ndistribution metrics. \nTable 3: Results of Linear Regression Model \n \nThe linear regression model does not sufficently capture the system complexity. \nThe second model trained was a random forest model (ensemble learning method for \nclassification/regression) ^REF Table 4 provides a summary of results given by the model for all the output \nvariables. The results are summarised using standard statistical distribution metrics. \n \nTable 4: Results of Random Forest Model \n \nSignificant non-linear relationships between input and output variables can be observed when using random \nforest (decision tree based) models.To improve the random forest models, input and output lags were \nutilised ranging from 60-115min intervals.  \nTable 5 below provides a summary of results given by the model for all the output variables. \n \nTable 5: Results of Random Forest Model using Input/ Output Lag Variables \n \nUsing input and output lagged values improves the R2  metrics. As we can see, random forest with lags \n(Table 5) results in a 260% improvement of the R2  metric when considering the 50th percentile results \n(Table 4 & 5). Further improvements were possible utilising the SME direction to aggregate banks of \nequipment into single variable representations (Table 6). \nTable 6 below is a summary of results given by the model for all the output variables.  \n \n \nTable 6: Results of Random Forest Model Using Input/ Output Lag Variables and Sum of Flow Variables \n \nThe Random Forest Model with input/output lags and equipment aggregation exhibited the best R2 values \nof all the models considered. \nModel Deployment \nOne model for each output variable was deployed using AWS utilising S3, SageMaker and Lambda to \ncreate a single multi-model endpoint. This presents a single model to the user/consumer to enable \nsimplified access and scalability (serverless) to run simulations of varying complexity / time horizons.The \nUI/UX utilised BootStrap JS and Kibana.The architecture/procedure-call flow chart  is shown \ndiagramatically in Figure 3. \n \nUSE-CASE RESULTS \n \nUse Case 1 relating to the prediction of asset utilisation above name plate was demonstrated by running \nthe model with hypothetical input data sets describing future load scenarios. \nUse Case 2 relating to the prediction of plant state 60 minutes in advance was tested using similar \ntechniques to Use Case 1 except with input data likely to generate excursions beyond equipment name \nplate capacity. \n \nOPPORTUNITIES FOR ENHANCEMENT \nFuture enhancement opportunities exist for example the exploration of RNNs (recurrent neural networks) \nas well as expansion of the UX/UI functionality developed around input data set definition and configuration \n- this would enable rapid scenario analysis. \n \nCONCLUSION \nThe aims of the project were achieved and it was demonstrated that machine learning techniques have a \nsignificant role to play in the automated model building arena providing the foundation for powerful, value-\ndriven Use-Cases. A relatively high degree of confidence was achieved (R^2 of 0.76) but areas for \nimprovement were identified for future exploration.implementation. \n \n', <faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001A7DC1F9710> >, ['APPROACH AND RESULTS \n \nIntroduction \nFigure 1 shows the overall approach to build and visualise the machine learning model', 'A primary input to \nthe process is Distributed Control System (DCS) Historian data provided in the form of a large CSV files.The \ntime-series data set consisted of 422,449 rows with each row having 106 columns and covered a time \nperiod of approximately 4-years (January 2015 - January 2019) with temporal resolution of 5-minutes', '\nIn order to assess equipment utilisation beyond capacity, additional equipment rating data was sourced \nfrom electrical power distribution system records', '\nThe following sections describe in further detail the specific activities undertaken across the data \npreparation, model training, model testing, model optimisation, deployment and visualisation phases', '\n \nAPPROACH \nData Preparation \nBefore the data could be used for training the models, the data was inspected for quality (missing values \netc), as well as the presence of key input variables well understood to likely drive the process plant \nbehaviour', '  \nDomain Subject Matter Experts (SMEs) recommended 14 key input variables, also referred to as a feature \nset, which were likely to influence the performance of the operating plant and hence drive equipment power \nconsumption', 'The key input variables identified represent plant inflow (available at 5-minute intervals) as \nwell as flow quality parameters such as COD, NH3, Turbidity,  pH among others (available once every one \nor two days)', ' \nThere were a total of 91 output variables (labels) describing equipment power consumption all of which \nwere generally available at 5-minute intervals', 'The power consumption data was available in different forms \n(Watts, amps, VA etc) which were normalised during model development to enable comparison with with \nequipment rating data', ' \nBoth the input feature set and output labels contained missing values for some time instances', ' As the data \nis continuously changing in time series data sets, the missing feature values were substituted with the last \navailable value of the feature preceding the missing value', 'When training and testing machine learning \nmodels, it is important to maintain the integrity of the output labels to obtain valid results', 'For an output \nlabel, instances with missing values were discarded for both model training and testing', '\nTo enable the prediction of the plant state 60minutes in advance, a commonly technique is to train the \nmodel utilising lagged data sets', 'To achieve this, a new data set lagged by a defined period of time (60min-\n115min) is created as the basis for training this use-case specific model', 'Within this new data set, the output \nlabels also become features for the purpose predicting future plant states', '\nModel Development \nModel development involved three main activities: training the model, testing the model and model \noptimization', '\nTraining and Testing Data \nThe data set was divided into a training and testing data set', 'The data set was divided into monthly datasets', '\nEach of the monthly datasets was then further divided into training and testing sets using a 90/10 split: 90% \nof data for training and 10% for testing', 'That resulted in data from the first 27 days of a month being in the \ntraining set and the remaining data in the testing set', 'The reasoning behind splitting the data set this way \nis to eliminate the introduction biasing of recent data over older data', 'A model was trained and tested for \neach month and for each output variable', '\nDefining an Appropriate Performance Metric \nThe commonly used coefficient of determination (R2) was used to assess model performance', 'The \ncoefficient of determination is used to assess how well the model predicts the real behaviour of the \nsystem', 'An R2 value of 0 indicates that the model does not explain any of the system behaviour, whilst a \nvalue of 1 indicates the model perfectly predicts the system behaviour', ' \nAssessment of Co-Variance Influences \nBefore model training, it was necessary to determine which of the input variables had high covariances with \nthe output variables', ""To assess this, Pearson's R (correlation coefficient) was calculated for each input-\noutput pair"", ""This did not yield many input/outputs pairs exhibiting a high Pearson's R correlation coefficient"", '\nTable 1 shows the top correlations found between input and output variables', '\n \nTable 1: Correlation Between Input and Output Variables \n \nAs shown in Table 1, there was no significant relationships identified between specific input-output pairs', '\nTo proceed with the modelling exericse, addition input was sought from SMEs to identify any additional \nfeatures that could be utilised to improve the peformance', 'The SMEs suggested that due to the nature and \narrangement of equipment items into ""banks"", it may be necessary to describe these groups as single \nvariables to  be modelled (by summation) rather than modelling individually on their own (for example banks \nof Direct On-Line loads)', '\n \nTable 2: Correlation Results from Summing Input and Output \n \n \nThe results in Table 2 show a significant improvement in the correlation values compared to the correlations \nshown in Table 1', '\nModel Training \nAs distinct from the Model Performance (R2), the first model  trained was a linear regression model (line-\nof-best fit)', 'Table 3 provides a summary of Performance Metric (R2) results of the regression models for all \nof the output variables across the testing data set', 'The results are summarised using standard statistical \ndistribution metrics', '\nTable 3: Results of Linear Regression Model \n \nThe linear regression model does not sufficently capture the system complexity', '\nThe second model trained was a random forest model (ensemble learning method for \nclassification/regression) ^REF Table 4 provides a summary of results given by the model for all the output \nvariables', 'The results are summarised using standard statistical distribution metrics', '\n \nTable 4: Results of Random Forest Model \n \nSignificant non-linear relationships between input and output variables can be observed when using random \nforest (decision tree based) models.To improve the random forest models, input and output lags were \nutilised ranging from 60-115min intervals', ' \nTable 5 below provides a summary of results given by the model for all the output variables', '\n \nTable 5: Results of Random Forest Model using Input/ Output Lag Variables \n \nUsing input and output lagged values improves the R2  metrics', 'As we can see, random forest with lags \n(Table 5) results in a 260% improvement of the R2  metric when considering the 50th percentile results \n(Table 4 & 5)', 'Further improvements were possible utilising the SME direction to aggregate banks of \nequipment into single variable representations (Table 6)', '\nTable 6 below is a summary of results given by the model for all the output variables', ' \n \n \nTable 6: Results of Random Forest Model Using Input/ Output Lag Variables and Sum of Flow Variables \n \nThe Random Forest Model with input/output lags and equipment aggregation exhibited the best R2 values \nof all the models considered', '\nModel Deployment \nOne model for each output variable was deployed using AWS utilising S3, SageMaker and Lambda to \ncreate a single multi-model endpoint', 'This presents a single model to the user/consumer to enable \nsimplified access and scalability (serverless) to run simulations of varying complexity / time horizons.The \nUI/UX utilised BootStrap JS and Kibana.The architecture/procedure-call flow chart  is shown \ndiagramatically in Figure 3', '\n \nUSE-CASE RESULTS \n \nUse Case 1 relating to the prediction of asset utilisation above name plate was demonstrated by running \nthe model with hypothetical input data sets describing future load scenarios', '\nUse Case 2 relating to the prediction of plant state 60 minutes in advance was tested using similar \ntechniques to Use Case 1 except with input data likely to generate excursions beyond equipment name \nplate capacity', '\n \nOPPORTUNITIES FOR ENHANCEMENT \nFuture enhancement opportunities exist for example the exploration of RNNs (recurrent neural networks) \nas well as expansion of the UX/UI functionality developed around input data set definition and configuration \n- this would enable rapid scenario analysis', '\n \nCONCLUSION \nThe aims of the project were achieved and it was demonstrated that machine learning techniques have a \nsignificant role to play in the automated model building arena providing the foundation for powerful, value-\ndriven Use-Cases', 'A relatively high degree of confidence was achieved (R^2 of 0.76) but areas for \nimprovement were identified for future exploration.implementation', '\n \n'])",,,2024-06-24 16:07:17.627474
,,,,,2024-06-24 16:11:03.084269
